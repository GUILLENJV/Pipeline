{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNHPW+2bSZO/0fRRk0IfhSE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GUILLENJV/Pipeline/blob/master/PS_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==2.8"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "85egZyc5xRk8",
        "outputId": "a9c03c46-39ab-414e-9877-216178fd01b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow==2.8\n",
            "  Downloading tensorflow-2.8.0-cp310-cp310-manylinux2010_x86_64.whl (497.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m497.6/497.6 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (24.3.7)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (3.9.0)\n",
            "Collecting keras-preprocessing>=1.1.1 (from tensorflow==2.8)\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (16.0.6)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (3.3.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (4.10.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (1.14.1)\n",
            "Collecting tensorboard<2.9,>=2.8 (from tensorflow==2.8)\n",
            "  Downloading tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tf-estimator-nightly==2.8.0.dev2021122109 (from tensorflow==2.8)\n",
            "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.5/462.5 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras<2.9,>=2.8.0rc0 (from tensorflow==2.8)\n",
            "  Downloading keras-2.8.0-py2.py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (0.36.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (1.62.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.8) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (2.27.0)\n",
            "Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.9,>=2.8->tensorflow==2.8)\n",
            "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (3.5.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (2.31.0)\n",
            "Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard<2.9,>=2.8->tensorflow==2.8)\n",
            "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0 (from tensorboard<2.9,>=2.8->tensorflow==2.8)\n",
            "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8) (1.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=0.11.15->tensorboard<2.9,>=2.8->tensorflow==2.8) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8) (3.2.2)\n",
            "Installing collected packages: tf-estimator-nightly, tensorboard-plugin-wit, keras, tensorboard-data-server, keras-preprocessing, google-auth-oauthlib, tensorboard, tensorflow\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.15.0\n",
            "    Uninstalling keras-2.15.0:\n",
            "      Successfully uninstalled keras-2.15.0\n",
            "  Attempting uninstall: tensorboard-data-server\n",
            "    Found existing installation: tensorboard-data-server 0.7.2\n",
            "    Uninstalling tensorboard-data-server-0.7.2:\n",
            "      Successfully uninstalled tensorboard-data-server-0.7.2\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 1.2.0\n",
            "    Uninstalling google-auth-oauthlib-1.2.0:\n",
            "      Successfully uninstalled google-auth-oauthlib-1.2.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.15.2\n",
            "    Uninstalling tensorboard-2.15.2:\n",
            "      Successfully uninstalled tensorboard-2.15.2\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.15.0\n",
            "    Uninstalling tensorflow-2.15.0:\n",
            "      Successfully uninstalled tensorflow-2.15.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pandas-gbq 0.19.2 requires google-auth-oauthlib>=0.7.0, but you have google-auth-oauthlib 0.4.6 which is incompatible.\n",
            "tf-keras 2.15.1 requires tensorflow<2.16,>=2.15, but you have tensorflow 2.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed google-auth-oauthlib-0.4.6 keras-2.8.0 keras-preprocessing-1.1.2 tensorboard-2.8.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.8.0 tf-estimator-nightly-2.8.0.dev2021122109\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "keras",
                  "tensorboard",
                  "tensorflow"
                ]
              },
              "id": "9d7ba5acc7a7453c8c5a9ed7d89ffb37"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tyuIvvvRydrj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow"
      ],
      "metadata": {
        "id": "DA-O8i7pxFmj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor"
      ],
      "metadata": {
        "id": "lR1Ib17qyitB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install scikeras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BggiYhtWVbg5",
        "outputId": "16efaa4c-da03-4266-a4f1-826335f4bb0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikeras in /usr/local/lib/python3.10/dist-packages (0.12.0)\n",
            "Requirement already satisfied: packaging>=0.21 in /usr/local/lib/python3.10/dist-packages (from scikeras) (24.0)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from scikeras) (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras) (3.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import scikeras\n",
        "from scikeras.wrappers import KerasRegressor"
      ],
      "metadata": {
        "id": "wVXo65D5Vq0J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z6sE-PuNvUp2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.model_selection import cross_val_score, KFold\n",
        "from sklearn.feature_selection import SelectKBest, f_regression\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "\n",
        "# pipeline\n",
        "from sklearn.compose import make_column_selector as selector\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# models\n",
        "from sklearn.linear_model import Ridge\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# nn\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "\n",
        "#load and save\n",
        "import pickle\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.compose import TransformedTargetRegressor\n",
        "\n",
        "class FeatureCreation(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self):\n",
        "        # print('FeatureCreation initialized')\n",
        "        return None\n",
        "\n",
        "    # For the fit method, we will pass the parameter x. This is our independent variables.\n",
        "    # This fit method will be called when we fit the pipeline.\n",
        "    def fit(self, x, y=None):\n",
        "        # print('Fit FeatureCreation called')\n",
        "        return self\n",
        "\n",
        "    # Here, we will perform all of our transformations. For creating features automatically, we could create parameters in the class and pass the column names to them.\n",
        "    # But in this case, since it's for this dataset specific, we will perform transformations in the column names directly into the fit method.\n",
        "    # The transform method is called when we caled fit and also predict in the Pipeline. And that makes sense, since we need to create our features when we will train and when we will predict.\n",
        "    def transform(self, x, y=None):\n",
        "        # print('Transform FeatureCreation called')\n",
        "        # creating a copy to avoid changes to the original dataset\n",
        "        x_ = x.copy()\n",
        "        # print(f'Before Transformation: {x_.shape}')\n",
        "        # and now, we create everyone of our features.\n",
        "        # Area power of two\n",
        "        x_['area2'] = x_['area'] ** 2\n",
        "        # The ratio between area and rooms\n",
        "        x_['area/room'] = x_['area'] / x_['rooms']\n",
        "        # The ratio between area and bathroom\n",
        "        x_['area/bathroom'] = x_['area'] / x_['bathroom']\n",
        "        # the sum of rooms and bathrooms\n",
        "        x_['rooms+bathroom'] = x_['rooms'] + x_['bathroom']\n",
        "        # the product between rooms and bathrooms\n",
        "        x_['rooms*bathroom'] = x_['rooms'] * x_['bathroom']\n",
        "        # the ratio between rooms and bathrooms\n",
        "        x_['rooms/bathroom'] = x_['rooms'] / x_['bathroom']\n",
        "        # the product between hoa and property tax\n",
        "        x_['hoa*property tax'] = x_['hoa (R$)'] * x_['property tax (R$)']\n",
        "        # print(f'After Transformation: {x_.shape}')\n",
        "        return x_"
      ],
      "metadata": {
        "id": "5wS55r--zZSN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df = pd.read_csv('houses_to_rent_v2_fteng.csv')\n",
        "df = pd.read_csv('houses_to_rent_v2_fteng.csv')"
      ],
      "metadata": {
        "id": "wP7Mu_BMzfJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WsAkiS-ezpJr",
        "outputId": "80421e16-84e1-4ca9-a8f2-4b30f1ad0d85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8995, 11)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelo Pipeline"
      ],
      "metadata": {
        "id": "r9E0y1K5zt8p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = df.drop(columns=['rent amount (R$)'], axis=1)\n",
        "y = df['rent amount (R$)']\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,\n",
        "                                                    y,\n",
        "                                                    test_size=0.25,\n",
        "                                                    random_state=0)"
      ],
      "metadata": {
        "id": "aNytqBe1zsJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora, vamos a categorizar nuestras variables en numéricas o categóricas. Esto es necesario ya que la transformación de cada tipo es diferente.\n",
        "\n",
        "Ahora, crearemos las tuberías para cada tipo de variable."
      ],
      "metadata": {
        "id": "w61TjZk00SGx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Categorical\n",
        "catTransformer = Pipeline(steps=[\n",
        "    # For categorical variables, we will use onehotencoder.\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "# Numerical\n",
        "numTransformer = Pipeline(steps=[\n",
        "    # For numerical features we will use standardscaler because we have already treated the dataset for outliers.\n",
        "    ('scaler', StandardScaler())\n",
        "])"
      ],
      "metadata": {
        "id": "T6Br9c_M0S1K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finalmente, vamos a integrar esos pipelines con un ColumnTransformer y crear nuestro preprocesador. Cada vez que queramos predecir, se aplicará este preprocesador."
      ],
      "metadata": {
        "id": "bizmih8Q0eQ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('numeric', numTransformer, selector(dtype_exclude=[\"category\", \"object\"])),\n",
        "        ('categoric', catTransformer, selector(dtype_include=[\"category\", \"object\"]))\n",
        "    ])"
      ],
      "metadata": {
        "id": "I4Apj6kD0XZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Métrica y Validación\n",
        "Nuestra variable objetivo es el importe del alquiler (R$), lo que significa que necesitamos un modelo de aprendizaje automático supervisado y nuestro objetivo es una variable continua, un problema de regresión.\n",
        "\n",
        "He elegido utilizar RMSE como nuestra métrica porque es más sensible a los valores atípicos que MAE, por lo que nos da una comprensión más amplia si eso está afectando a nuestro modelo.\n",
        "\n",
        "Para la validación, vamos a utilizar K-Fold Cross Validation. Esto significa que los datos se dividirán en K grupos de muestras, llamados pliegues. Entonces, en cada iteración de K, los datos serán entrenados en K-1 y probados en el resto. A continuación tenemos un ejemplo de cómo funciona K-Fold, según la documentación de sci-kit learn."
      ],
      "metadata": {
        "id": "eFBAE8so10tI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We will use 5 folds\n",
        "n_folds = 5\n",
        "\n",
        "\n",
        "def rmsle_cv(model):\n",
        "    kf = KFold(n_folds, shuffle=True).get_n_splits(df.values)\n",
        "    # here we define that our scoring metric will be rmse for every iteration of the cross validation\n",
        "    rmse = np.sqrt(-cross_val_score(model, x_train, y_train,\n",
        "                   scoring=\"neg_mean_squared_error\", cv=kf))\n",
        "    return rmse"
      ],
      "metadata": {
        "id": "7jw2-AUM2JVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Validación de modelos\n",
        "Ahora tenemos que definir unos cuantos modelos base para validar nuestra métrica.\n",
        "\n",
        "También haremos una búsqueda aleatoria para ajustar nuestros modelos. Decidimos utilizar la búsqueda aleatoria en lugar de la búsqueda de cuadrícula porque las investigaciones muestran que la búsqueda aleatoria hace un mejor trabajo.\n",
        "\n",
        "Para la línea de base vamos a utilizar estos modelos:\n",
        "\n",
        "* RandomForest\n",
        "* XGBoost\n",
        "* LGBM\n",
        "* Ridge\n",
        "* Red neuronal\n",
        "\n",
        "Vamos a crear la topología de la red neuronal.\n",
        "\n",
        "El número de capas ocultas y neuronas es un factor importante para que el modelo generalice bien.\n",
        "\n",
        "De Introduction to Neural Networks for Java (segunda edición) por Jeff Heaton, hay dos decisiones que deben tomarse con respecto a las capas ocultas:\n",
        "\n",
        "1 - Cuántas capas ocultas tener realmente en la red neuronal .\n",
        "\n",
        "* Los problemas que requieren dos o más capas ocultas rara vez se encuentran.\n",
        "\n",
        "2 - Cuántas neuronas habrá en cada una de estas capas.\n",
        "\n",
        "* El número de neuronas ocultas debe estar entre el tamaño de la capa de entrada y el tamaño de la capa de salida.\n",
        "* El número de neuronas ocultas debe ser 2/3 del tamaño de la capa de entrada, más el tamaño de la capa de salida.\n",
        "* El número de neuronas ocultas debe ser inferior al doble del tamaño de la capa de entrada.\n"
      ],
      "metadata": {
        "id": "-gWxlBc72X0g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(optimizer='adam', dropout=0.2, activation='relu', kernel_initializer='normal'):\n",
        "    # We initialized every parameter that we want to optimize in the neural network.\n",
        "    # Let's initialize our sequential model\n",
        "    model = Sequential()\n",
        "    # Our input layer will have 15 neurons, that's equal to the input_dim\n",
        "    model.add(Dense(units = 15, activation = activation, input_dim = 15, kernel_initializer=kernel_initializer))\n",
        "    # In every step we will create a dropout layer in order to optimize this parameter and prevent overfitting.\n",
        "    model.add(Dropout(dropout))\n",
        "    # Ni = number of input neurons\n",
        "    # No = number of output neurons\n",
        "    # Our nn will have only one hidden layer, and the number of neurons follow the rule: 2/3 * (Ni) + No  = 11.\n",
        "    model.add(Dense(units = 11, activation = activation))\n",
        "    model.add(Dropout(dropout))\n",
        "    # Our output layer have only one neuron, since it's a regression problem.\n",
        "    model.add(Dense(units = 1, activation = activation))\n",
        "\n",
        "    # now we compile our model utilizing the mean squared error loss function\n",
        "    model.compile(optimizer = optimizer, loss = 'mean_squared_error', metrics = ['accuracy'])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "PpHyJVuR0LXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tempfile import mkdtemp\n",
        "from shutil import rmtree\n",
        "\n",
        "def get_best_param(model, custom_features=False, custom_target=False):\n",
        "    \"\"\"\n",
        "    This function return a random search object.\n",
        "    \"\"\"\n",
        "    # cachedir = mkdtemp()\n",
        "\n",
        "    # The model will be identified by a string and for each one, we will set a parameter grid. This grid will be passed to the random search\n",
        "    # Defined the model and the parameter grid, we instantiate the Pipeline.\n",
        "    # Since we are using a Pipeline, we have to optimize the parameters of our model, and to do that we will have to name the step in the pipeline and access that in the pipeline.\n",
        "    # In our case, our step will be named model. To access this parameters we will add a model__ in front of every parameter.\n",
        "    if model == 'RandomForest':\n",
        "        random_grid = {\n",
        "            'model__n_estimators': [int(x) for x in np.linspace(start = 200, stop = 800, num = 4)],\n",
        "            'model__max_features': ['auto', 'sqrt'],\n",
        "            'model__max_depth': [i for i in np.arange(1, 10)],\n",
        "            'model__min_samples_split': [2, 5, 10],\n",
        "            'model__min_samples_leaf': [1, 2, 4],\n",
        "            'model__bootstrap': [True, False]\n",
        "        }\n",
        "\n",
        "        regressor_model = RandomForestRegressor()\n",
        "\n",
        "    elif model == 'XGB':\n",
        "        random_grid = {\n",
        "            \"model__n_estimators\":[int(x) for x in np.linspace(start = 200, stop = 800, num = 4)],\n",
        "            \"model__learning_rate\"    : [0.05, 0.10, 0.15, 0.20, 0.25, 0.30 ],\n",
        "            \"model__max_depth\"        : [i for i in np.arange(1, 10)],\n",
        "            \"model__min_child_weight\" : [1e-3, 1, 3, 5, 7 ],\n",
        "            \"model__gamma\"            : [ 0.0, 0.1, 0.2 , 0.3, 0.4 ],\n",
        "            \"model__colsample_bytree\" : [ 0.3, 0.4, 0.5 , 0.7, 1 ]\n",
        "        }\n",
        "\n",
        "        regressor_model = xgb.XGBRegressor()\n",
        "\n",
        "    elif model == 'LGBM':\n",
        "        random_grid = {\n",
        "            \"model__n_estimators\": [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)],\n",
        "            \"model__boosting_type\": ['dart', 'goss'],\n",
        "            \"model__max_depth\": [i for i in np.arange(1, 51)],\n",
        "            \"model__num_leaves\": [int(x) for x in np.linspace(start = 10, stop = 2000, num = 10)],\n",
        "            \"model__learning_rate\"    : [0.05, 0.10, 0.15, 0.20, 0.25, 0.30 ],\n",
        "            \"model__min_child_weight\" : [1e-3, 1, 3, 5, 7 ],\n",
        "            \"model__colsample_bytree\" : [ 0.3, 0.4, 0.5 , 0.7, 1 ],\n",
        "        }\n",
        "\n",
        "        regressor_model = lgb.LGBMRegressor()\n",
        "\n",
        "    elif model == 'Ridge':\n",
        "        random_grid = {\n",
        "            \"model__alpha\": np.linspace(start=0.001, stop=1,  num=101),\n",
        "            \"model__fit_intercept\": [True, False]\n",
        "        }\n",
        "\n",
        "        regressor_model = Ridge()\n",
        "\n",
        "    elif model == 'NeuralNetwork':\n",
        "        random_grid = {\n",
        "            \"model__batch_size\": (32, 64, 128, 256),\n",
        "            \"model__epochs\": (50, 100, 200, 300),\n",
        "            \"model__activation\": ('relu', 'tanh', 'linear'),\n",
        "            \"model__dropout\": (0.0, 0.1, 0.2, 0.3),\n",
        "            \"model__kernel_initializer\": ('glorot_uniform','normal','uniform'),\n",
        "            \"model__optimizer\": ('SGD', 'Adam', 'RMSprop', 'Adadelta', 'Adagrad', 'Adamax', 'Nadam', 'Ftrl')\n",
        "        }\n",
        "\n",
        "\n",
        "        # Since we are passing our model to a sci-kit learn Pipeline, we need to wrap our Keras model first.\n",
        "        regressor_model = KerasRegressor(build_fn=create_model, verbose=0)\n",
        "\n",
        "    # For the neural network, we have a particularity. We have to pass to the first layer the input dimension.\n",
        "    # Since we are doing transformations in our dataset, such as One Hot encoding, we don't know for sure how many features will exist.\n",
        "    # One way around that it is define a fix value of variables that will be passed, and this is possible by using SelecKBest from sci-kit learn.\n",
        "    # This function scores the variables according to the function passed, in our case, f_regression, and return the k variables defined.\n",
        "    # Defined the number of features, we just pass the input dim in our create_model function above and create another step in our Pipeline.\n",
        "\n",
        "\n",
        "    # We will test for every possible combination regarding to the target transformation and the feature engineering and compute the results.\n",
        "    if custom_features:\n",
        "        if model == 'NeuralNetwork':\n",
        "            select_best_features = SelectKBest(k=15, score_func=f_regression)\n",
        "            model_pipeline = Pipeline(steps=[\n",
        "                        ('featurecreation', FeatureCreation()),\n",
        "                        ('preprocessor', preprocessor),\n",
        "                        ('select_k_best', select_best_features),\n",
        "                        ('model', regressor_model)\n",
        "                    ])\n",
        "        else:\n",
        "            model_pipeline = Pipeline(steps=[\n",
        "                        ('featurecreation', FeatureCreation()),\n",
        "                        ('preprocessor', preprocessor),\n",
        "                        ('model', regressor_model)\n",
        "                    ])\n",
        "    else:\n",
        "        if model == 'NeuralNetwork':\n",
        "            select_best_features = SelectKBest(k=15, score_func=f_regression)\n",
        "            model_pipeline = Pipeline(steps=[\n",
        "                        ('preprocessor', preprocessor),\n",
        "                        ('select_k_best', select_best_features),\n",
        "                        ('model', regressor_model)\n",
        "                    ])\n",
        "        else:\n",
        "            model_pipeline = Pipeline(steps=[\n",
        "                        ('preprocessor', preprocessor),\n",
        "                        ('model', regressor_model)\n",
        "                    ])\n",
        "\n",
        "    kf = KFold(n_folds, shuffle=True).get_n_splits(df.values)\n",
        "\n",
        "    if custom_target:\n",
        "        custom_pipeline = TransformedTargetRegressor(\n",
        "            regressor=model_pipeline,\n",
        "            func=np.log,\n",
        "            inverse_func=np.exp\n",
        "            )\n",
        "\n",
        "        for old_key in list(random_grid.keys()):\n",
        "            random_grid['regressor__' + old_key] = random_grid.pop(old_key)\n",
        "        rzsearch = RandomizedSearchCV(estimator=custom_pipeline, param_distributions=random_grid, cv=kf, n_jobs=-1)\n",
        "    else:\n",
        "        rzsearch = RandomizedSearchCV(estimator=model_pipeline, param_distributions=random_grid, cv=kf, n_jobs=-1)\n",
        "\n",
        "    return rzsearch\n",
        "# Now, let's iterate through every model, run the random search in each one and return a dataframe with the informations.\n",
        "def result_cv_models(custom_features=False, custom_target=False):\n",
        "    today = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "    models = ['RandomForest', 'XGB', 'LGBM', 'Ridge', 'NeuralNetwork']\n",
        "    best_models = dict()\n",
        "    results_dict = dict()\n",
        "\n",
        "    if custom_target:\n",
        "        for model in models:\n",
        "            rzsearch = get_best_param(model, custom_features=custom_features, custom_target=custom_target).fit(x_train, y_train)\n",
        "            best_models[model] = (rzsearch.best_estimator_.regressor_['model'], [rzsearch.best_params_])\n",
        "    else:\n",
        "        for model in models:\n",
        "            rzsearch = get_best_param(model, custom_features=custom_features, custom_target=custom_target).fit(x_train, y_train)\n",
        "            best_models[model] = (rzsearch.best_estimator_['model'], [rzsearch.best_params_])\n",
        "\n",
        "    for name, model in best_models.items():\n",
        "\n",
        "        if custom_features:\n",
        "            if name == 'NeuralNetwork':\n",
        "                select_best_features = SelectKBest(k=15, score_func=f_regression)\n",
        "                model_pipeline = Pipeline(steps=[\n",
        "                            ('featurecreation', FeatureCreation()),\n",
        "                            ('preprocessor', preprocessor),\n",
        "                            ('select_k_best', select_best_features),\n",
        "                            ('model', model[0])\n",
        "                        ])\n",
        "            else:\n",
        "                model_pipeline = Pipeline(steps=[\n",
        "                            ('featurecreation', FeatureCreation()),\n",
        "                            ('preprocessor', preprocessor),\n",
        "                            ('model', model[0])\n",
        "                        ])\n",
        "        else:\n",
        "            if name == 'NeuralNetwork':\n",
        "                select_best_features = SelectKBest(k=15, score_func=f_regression)\n",
        "                model_pipeline = Pipeline(steps=[\n",
        "                            ('preprocessor', preprocessor),\n",
        "                            ('select_k_best', select_best_features),\n",
        "                            ('model', model[0])\n",
        "                        ])\n",
        "            else:\n",
        "                model_pipeline = Pipeline(steps=[\n",
        "                            ('preprocessor', preprocessor),\n",
        "                            ('model', model[0])\n",
        "                        ])\n",
        "\n",
        "        # If we are performing a target transformation, we have to pass the pipeline to our TransformedTargetRegressor object.\n",
        "        if custom_target:\n",
        "            custom_pipeline = TransformedTargetRegressor(\n",
        "                regressor=model_pipeline,\n",
        "                func=np.log,\n",
        "                inverse_func=np.exp\n",
        "                )\n",
        "\n",
        "            scores = rmsle_cv(custom_pipeline)\n",
        "\n",
        "            # save the model\n",
        "            custom_pipeline.fit(x_train, y_train)\n",
        "\n",
        "            predict_test = custom_pipeline.predict(x_test)\n",
        "\n",
        "            rmse_testset = mean_squared_error(y_test, predict_test, squared=False)\n",
        "            mae_testset = mean_absolute_error(y_test, predict_test)\n",
        "\n",
        "            # for the neural network we need additional steps.\n",
        "            if name == 'NeuralNetwork':\n",
        "                # The keras model it is not serialized by pickle. To get around that, we save the model using the keras save method.\n",
        "                # Similar to the pipeline object, to access the model, we have to dig into the steps of the transformed object, and then dig into the pipeline object.\n",
        "                # custom_pipeline.regressor_.named_steps['model'].model.save(os.path.join(os.path.abspath(''), \"models\", f'v1_model_{name}_{round(scores.mean(), 3)}_{today}.h5'))\n",
        "                custom_pipeline.regressor_.named_steps['model'].model.save(os.path.join(os.path.abspath('../models'), f'v1_model_{name}_{round(scores.mean(), 3)}_{today}.h5'))\n",
        "                # Then, we set the model inside the pipeline equals to None to be able to serialize.\n",
        "                custom_pipeline.regressor_.named_steps['model'].model = None\n",
        "\n",
        "                # Into the transformed object we have to set the regressor equals None also.\n",
        "                # Later, we will load the pipeline and the model and add the model to the pipeline again.\n",
        "                custom_pipeline.regressor.set_params(model = None)\n",
        "\n",
        "            # Now, serialize and save the model.\n",
        "            # with open(os.path.join(os.path.abspath(''), \"models\", f\"v1_pipe_{name}_{round(scores.mean(), 3)}_{today}.pickle\"), 'wb') as f:\n",
        "            with open(os.path.join(os.path.abspath('../models'), f\"v1_pipe_{name}_{round(scores.mean(), 3)}_{today}.pickle\"), 'wb') as f:\n",
        "                pickle.dump(custom_pipeline, f, -1)\n",
        "\n",
        "        else:\n",
        "            scores = rmsle_cv(model_pipeline)\n",
        "\n",
        "            #save the model\n",
        "            model_pipeline.fit(x_train, y_train)\n",
        "\n",
        "            predict_test = model_pipeline.predict(x_test)\n",
        "\n",
        "            rmse_testset = mean_squared_error(y_test, predict_test, squared=False)\n",
        "            mae_testset = mean_absolute_error(y_test, predict_test)\n",
        "\n",
        "            if name == 'NeuralNetwork':\n",
        "                # model_pipeline.named_steps['model'].model.save(os.path.join(os.path.abspath(''), \"models\", f'v1_model_{name}_{round(scores.mean(), 3)}_{today}.h5'))\n",
        "                model_pipeline.named_steps['model'].model.save(os.path.join(os.path.abspath('../models'), f'v1_model_{name}_{round(scores.mean(), 3)}_{today}.h5'))\n",
        "                model_pipeline.named_steps['model'].model = None\n",
        "\n",
        "            # with open(os.path.join(os.path.abspath(''), \"models\", f\"v1_pipe_{name}_{round(scores.mean(), 3)}_{today}.pickle\"), 'wb') as f:\n",
        "            with open(os.path.join(os.path.abspath('../models'), f\"v1_pipe_{name}_{round(scores.mean(), 3)}_{today}.pickle\"), 'wb') as f:\n",
        "                pickle.dump(model_pipeline, f, -1)\n",
        "\n",
        "        # Here we will save our results. One important column it is the 'pipe_file_name', this will be used to load our model later.\n",
        "        results_dict[name] = {'name': name, 'model': model[0], 'params': model[1], 'rmse_cv': round(np.mean(scores), 3), 'std_cv': round(np.std(scores), 3), 'rmse_testset': rmse_testset, 'mae_testset': mae_testset, 'custom_features': custom_features, 'custom_target': custom_target, 'all_scores_cv': scores, 'pipe_file_name': f\"v1_pipe_{name}_{round(scores.mean(), 3)}_{today}.pickle\"}\n",
        "\n",
        "    results_df = pd.DataFrame(results_dict).T\n",
        "    return results_df"
      ],
      "metadata": {
        "id": "4mwFUMn73OGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, let's iterate through every model, run the random search in each one and return a dataframe with the informations.\n",
        "def result_cv_models(custom_features=False, custom_target=False):\n",
        "    today = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "    models = ['RandomForest', 'XGB', 'LGBM', 'Ridge', 'NeuralNetwork']\n",
        "    best_models = dict()\n",
        "    results_dict = dict()\n",
        "\n",
        "    if custom_target:\n",
        "        for model in models:\n",
        "            rzsearch = get_best_param(model, custom_features=custom_features, custom_target=custom_target).fit(x_train, y_train)\n",
        "            best_models[model] = (rzsearch.best_estimator_.regressor_['model'], [rzsearch.best_params_])\n",
        "    else:\n",
        "        for model in models:\n",
        "            rzsearch = get_best_param(model, custom_features=custom_features, custom_target=custom_target).fit(x_train, y_train)\n",
        "            best_models[model] = (rzsearch.best_estimator_['model'], [rzsearch.best_params_])\n",
        "\n",
        "    for name, model in best_models.items():\n",
        "\n",
        "        if custom_features:\n",
        "            if name == 'NeuralNetwork':\n",
        "                select_best_features = SelectKBest(k=15, score_func=f_regression)\n",
        "                model_pipeline = Pipeline(steps=[\n",
        "                            ('featurecreation', FeatureCreation()),\n",
        "                            ('preprocessor', preprocessor),\n",
        "                            ('select_k_best', select_best_features),\n",
        "                            ('model', model[0])\n",
        "                        ])\n",
        "            else:\n",
        "                model_pipeline = Pipeline(steps=[\n",
        "                            ('featurecreation', FeatureCreation()),\n",
        "                            ('preprocessor', preprocessor),\n",
        "                            ('model', model[0])\n",
        "                        ])\n",
        "        else:\n",
        "            if name == 'NeuralNetwork':\n",
        "                select_best_features = SelectKBest(k=15, score_func=f_regression)\n",
        "                model_pipeline = Pipeline(steps=[\n",
        "                            ('preprocessor', preprocessor),\n",
        "                            ('select_k_best', select_best_features),\n",
        "                            ('model', model[0])\n",
        "                        ])\n",
        "            else:\n",
        "                model_pipeline = Pipeline(steps=[\n",
        "                            ('preprocessor', preprocessor),\n",
        "                            ('model', model[0])\n",
        "                        ])\n",
        "\n",
        "        # If we are performing a target transformation, we have to pass the pipeline to our TransformedTargetRegressor object.\n",
        "        if custom_target:\n",
        "            custom_pipeline = TransformedTargetRegressor(\n",
        "                regressor=model_pipeline,\n",
        "                func=np.log,\n",
        "                inverse_func=np.exp\n",
        "                )\n",
        "\n",
        "            scores = rmsle_cv(custom_pipeline)\n",
        "\n",
        "            # save the model\n",
        "            custom_pipeline.fit(x_train, y_train)\n",
        "\n",
        "            predict_test = custom_pipeline.predict(x_test)\n",
        "\n",
        "            rmse_testset = mean_squared_error(y_test, predict_test, squared=False)\n",
        "            mae_testset = mean_absolute_error(y_test, predict_test)\n",
        "\n",
        "            # for the neural network we need additional steps.\n",
        "            if name == 'NeuralNetwork':\n",
        "                # The keras model it is not serialized by pickle. To get around that, we save the model using the keras save method.\n",
        "                # Similar to the pipeline object, to access the model, we have to dig into the steps of the transformed object, and then dig into the pipeline object.\n",
        "                # custom_pipeline.regressor_.named_steps['model'].model.save(os.path.join(os.path.abspath(''), \"models\", f'v1_model_{name}_{round(scores.mean(), 3)}_{today}.h5'))\n",
        "                custom_pipeline.regressor_.named_steps['model'].model.save(os.path.join(os.path.abspath('../models'), f'v1_model_{name}_{round(scores.mean(), 3)}_{today}.h5'))\n",
        "                # Then, we set the model inside the pipeline equals to None to be able to serialize.\n",
        "                custom_pipeline.regressor_.named_steps['model'].model = None\n",
        "\n",
        "                # Into the transformed object we have to set the regressor equals None also.\n",
        "                # Later, we will load the pipeline and the model and add the model to the pipeline again.\n",
        "                custom_pipeline.regressor.set_params(model = None)\n",
        "\n",
        "            # Now, serialize and save the model.\n",
        "            # with open(os.path.join(os.path.abspath(''), \"models\", f\"v1_pipe_{name}_{round(scores.mean(), 3)}_{today}.pickle\"), 'wb') as f:\n",
        "            with open(os.path.join(os.path.abspath('../models'), f\"v1_pipe_{name}_{round(scores.mean(), 3)}_{today}.pickle\"), 'wb') as f:\n",
        "                pickle.dump(custom_pipeline, f, -1)\n",
        "\n",
        "        else:\n",
        "            scores = rmsle_cv(model_pipeline)\n",
        "\n",
        "            #save the model\n",
        "            model_pipeline.fit(x_train, y_train)\n",
        "\n",
        "            predict_test = model_pipeline.predict(x_test)\n",
        "\n",
        "            rmse_testset = mean_squared_error(y_test, predict_test, squared=False)\n",
        "            mae_testset = mean_absolute_error(y_test, predict_test)\n",
        "\n",
        "            if name == 'NeuralNetwork':\n",
        "                # model_pipeline.named_steps['model'].model.save(os.path.join(os.path.abspath(''), \"models\", f'v1_model_{name}_{round(scores.mean(), 3)}_{today}.h5'))\n",
        "                model_pipeline.named_steps['model'].model.save(os.path.join(os.path.abspath('../models'), f'v1_model_{name}_{round(scores.mean(), 3)}_{today}.h5'))\n",
        "                model_pipeline.named_steps['model'].model = None\n",
        "\n",
        "            # with open(os.path.join(os.path.abspath(''), \"models\", f\"v1_pipe_{name}_{round(scores.mean(), 3)}_{today}.pickle\"), 'wb') as f:\n",
        "            with open(os.path.join(os.path.abspath('../models'), f\"v1_pipe_{name}_{round(scores.mean(), 3)}_{today}.pickle\"), 'wb') as f:\n",
        "                pickle.dump(model_pipeline, f, -1)\n",
        "\n",
        "        # Here we will save our results. One important column it is the 'pipe_file_name', this will be used to load our model later.\n",
        "        results_dict[name] = {'name': name, 'model': model[0], 'params': model[1], 'rmse_cv': round(np.mean(scores), 3), 'std_cv': round(np.std(scores), 3), 'rmse_testset': rmse_testset, 'mae_testset': mae_testset, 'custom_features': custom_features, 'custom_target': custom_target, 'all_scores_cv': scores, 'pipe_file_name': f\"v1_pipe_{name}_{round(scores.mean(), 3)}_{today}.pickle\"}\n",
        "\n",
        "    results_df = pd.DataFrame(results_dict).T\n",
        "    return results_df"
      ],
      "metadata": {
        "id": "xp7UpW4m3eCv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_results_df = list()\n",
        "\n",
        "import time\n",
        "start = time.time()\n",
        "\n",
        "combinations = ({'custom_feature': True, 'custom_target': True}, {'custom_feature': True, 'custom_target': False}, {'custom_feature': False, 'custom_target': True}, {'custom_feature': False, 'custom_target': False})\n",
        "\n",
        "# We have 4 possible combinations, let's get the results of each one of them.\n",
        "for combination in combinations:\n",
        "    print(combination)\n",
        "    results_df = result_cv_models(custom_features=combination['custom_feature'], custom_target=combination['custom_target'])\n",
        "    all_results_df.append(results_df)\n",
        "\n",
        "end = time.time()\n",
        "time_run = end-start"
      ],
      "metadata": {
        "id": "IcNn4Cum3npv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "time_run"
      ],
      "metadata": {
        "id": "-6jbpRPKXNJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df = pd.concat(i for i in all_results_df)"
      ],
      "metadata": {
        "id": "_D4zA18DXN3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df.to_csv(('model_evaluation.csv'), index=False)"
      ],
      "metadata": {
        "id": "QLjFUttzXPah"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}